# -*- coding: utf-8 -*-
"""Bike_Share_Python.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mFBddy7sSEc3gLXajY2agwMLpIimHKpa

**Importing all Required Libraries**
"""

import pylab
import calendar
import numpy as np
import pandas as pd
import seaborn as sn
from scipy import stats
import missingno as msno
from datetime import datetime
import matplotlib.pyplot as plt
import warnings
pd.options.mode.chained_assignment = None
warnings.filterwarnings("ignore", category=DeprecationWarning)
# %matplotlib inline
from google.colab import files
import io

"""**Importing the data set and creating a dataframe object for the data**"""

url1 = "https://drive.google.com/uc?export=download&id=1_3NMAGeupfl_8BfSJ5ep-TD_NA0zqFCx"
url2 = "https://drive.google.com/uc?export=download&id=13Z1ohEvuZOMMP9pKxinjpEupzQIuZMgo"

data = pd.read_csv(url1, header=0)
data_weather = data.copy()

"""**Perform Basic Analysis of the data**
**1. Look at the datatypes**
**2. Structure of the data**
**3. Values in the Data**
"""

data.dtypes
data.head()
data.shape

"""Columns "season","holiday","workingday" and "weather" should be of "categorical" data type.But the current data type is "int" for those columns. This needs transformation part of EDA

    1. Create new columns "date,"hour","weekDay","month" from "datetime" column.
    2. Coerce the datatype of "season","holiday","workingday" and weather to category type.
    3. Drop the datetime column after completion of step 2
"""

data["date"] = data.datetime.apply(lambda x : x.split()[0])
data["hour"] = data.datetime.apply(lambda x : x.split()[1].split(":")[0])
data["weekday"] = data.date.apply(lambda dateString : calendar.day_name[datetime.strptime(dateString,"%m/%d/%Y").weekday()])
data["month"] = data.date.apply(lambda dateString : calendar.month_name[datetime.strptime(dateString,"%m/%d/%Y").month])

"""**Recoding to the actual meaning of the season and weather feature**"""

data["season"] = data.season.map({1: "Spring", 2 : "Summer", 3 : "Fall", 4 :"Winter" })
data["weather"] = data.weather.map({1: " Clear + Few clouds + Partly cloudy + Partly cloudy",\
                                        2 : " Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist ", \
                                        3 : " Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds", \
                                        4 :" Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog " })

cat1 = ["hour","weekday","month","season","weather","holiday","workingday"]
for x in cat1:
    data[x] = data[x].astype("category")

data =data.drop(["datetime"],axis=1)

"""**Checking for Missing Values**"""

data.isnull().sum()

fig, axes = plt.subplots(nrows=2,ncols=2)
fig.set_size_inches(12, 10)
sn.boxplot(data=data,y="count",orient="v",ax=axes[0][0])
sn.boxplot(data=data,y="count",x="season",orient="v",ax=axes[0][1])
sn.boxplot(data=data,y="count",x="hour",orient="v",ax=axes[1][0])
sn.boxplot(data=data,y="count",x="workingday",orient="v",ax=axes[1][1])

axes[0][0].set(ylabel='Count',title="Box Plot On Count")
axes[0][1].set(xlabel='Season', ylabel='Count',title="Box Plot On Count Across Season")
axes[1][0].set(xlabel='Hour Of The Day', ylabel='Count',title="Box Plot On Count Across Hour Of The Day")
axes[1][1].set(xlabel='Working Day', ylabel='Count',title="Box Plot On Count Across Working Day")

"""**Outlier Analysis **

1. "count" variable contains lot of outlier data points which skews the distribution towards right (as there are more data points beyond Outer Quartile Limit.

2. Spring season has got relatively lower count.The dip in median value in boxplot gives evidence for it.

3. The median value for the boxplot with "Hour Of The Day" are higher at 7AM - 8AM and 5PM - 6PM.  This could be because of the peak overs for school and office goers.

4.Most of the outlier points are mainly contributed from "Working Day" than "Non Working Day".

**Correlation Analysis**
"""

corr = data[["temp","atemp","casual","registered","humidity","windspeed","count"]].corr()
mask = np.array(corr)
mask[np.tril_indices_from(mask)] = False
fig,ax= plt.subplots()
fig.set_size_inches(20,10)
sn.heatmap(corr, mask=mask,vmax=.8, square=True,annot=True)

"""One common to understand how a dependent variable is influenced by features (numerical) is to fibd a correlation matrix between them. Lets plot a correlation plot between "count" and ["temp","atemp","humidity","windspeed"].

1.temp and humidity features has got positive and negative correlation with count respectively.Although the correlation between them are not very prominent still the count variable has got little dependency on "temp" and "humidity".
2.windspeed is not gonna be really useful numerical feature and it is visible from it correlation value with "count"
"atemp" is variable is not taken into since "atemp" and "temp" has got strong correlation with each other. During model building any one of the variable has to be dropped since they will exhibit multicollinearity in the data.
3."Casual" and "Registered" are also not taken into account since they could be leakage variables by nature and need to dropped during model building.
"""

import matplotlib.pyplot as plt
fig,(ax1,ax2,ax3) = plt.subplots(ncols=4)
fig.set_size_inches(12, 5)
sn.regplot(x="temp", y="count", data=data,ax=ax1)
sn.regplot(x="windspeed", y="count", data = data,ax=ax2)
sn.regplot(x="humidity", y="count", data = data, ax=ax3)

"""Variation of count variables with temp,windspeed and humidity. From the plot we can see that"""

fig,axes = plt.subplots(ncols=2,nrows=1)
fig.set_size_inches(12, 10)
sn.distplot(data["count"],ax=axes[0])
stats.probplot(data["count"], dist='norm', fit=True, plot=axes[1])

data.head()
data["hour"] = pd.to_numeric(data["hour"])
data.dtypes

fig,(ax1,ax2,ax3,ax4)= plt.subplots(nrows=4)
fig.set_size_inches(12,20)
sortOrder = ["January","February","March","April","May","June","July","August","September","October","November","December"]
hueOrder = ["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]

monthAggregated = pd.DataFrame(data.groupby("month")["count"].mean()).reset_index()
monthSorted = monthAggregated.sort_values(by="count",ascending=False)
sn.barplot(data=monthSorted,x="month",y="count",ax=ax1,order=sortOrder)
ax1.set(xlabel='Month', ylabel='Avearage Count',title="Average Count By Month")

hourAggregated = pd.DataFrame(data.groupby(["hour","season"],sort=True)["count"].mean()).reset_index()
sn.pointplot(x=hourAggregated["hour"], y=hourAggregated["count"],hue=hourAggregated["season"], data=hourAggregated, join=True,ax=ax2)
ax2.set(xlabel='Hour Of The Day', ylabel='Users Count',title="Average Users Count By Hour Of The Day Across Season",label='big')

hourAggregated = pd.DataFrame(data.groupby(["hour","weekday"],sort=True)["count"].mean()).reset_index()
sn.pointplot(x=hourAggregated["hour"], y=hourAggregated["count"],hue=hourAggregated["weekday"],hue_order=hueOrder, data=hourAggregated, join=True,ax=ax3)
ax3.set(xlabel='Hour Of The Day', ylabel='Users Count',title="Average Users Count By Hour Of The Day Across Weekdays",label='big')

hourTransformed = pd.melt(data[["hour","casual","registered"]], id_vars=['hour'], value_vars=['casual', 'registered'])
hourAggregated = pd.DataFrame(hourTransformed.groupby(["hour","variable"],sort=True)["value"].mean()).reset_index()
sn.pointplot(x=hourAggregated["hour"], y=hourAggregated["value"],hue=hourAggregated["variable"],hue_order=["casual","registered"], data=hourAggregated, join=True,ax=ax4)
ax4.set(xlabel='Hour Of The Day', ylabel='Users Count',title="Average Users Count By Hour Of The Day Across User Type",label='big')



"""It is quiet obvious that people tend to rent bike during summer season since it is really conducive to ride bike at that season.Therefore June, July and August has got relatively higher demand for bicycle.
On weekdays more people tend to rent bicycle around 7AM-8AM and 5PM-6PM. As we mentioned earlier this can be attributed to regular school and office commuters.
Above pattern is not observed on "Saturday" and "Sunday".More people tend to rent bicycle between 10AM and 4PM.
The peak user count around 7AM-8AM and 5PM-6PM is purely contributed by registered user
"""

datatrain = pd.read_csv(url1, header=0)
datatest = pd.read_csv(url2, header=0)

data_final = datatrain.append(datatest)
data_final.reset_index(inplace=True)
data_final.drop('index',inplace=True,axis=1)

data_final

data_final["date"] = data_final.datetime.apply(lambda x : x.split()[0])
data_final["hour"] = data_final.datetime.apply(lambda x : x.split()[1].split(":")[0]).astype("int")
data_final["year"] = data_final.datetime.apply(lambda x : x.split()[0].split("-")[0])
data_final["weekday"] = data_final.date.apply(lambda dateString : datetime.strptime(dateString,"%m/%d/%Y").weekday())
data_final["month"] = data_final.date.apply(lambda dateString : datetime.strptime(dateString,"%m/%d/%Y").month)

data_final.dtypes

catFeatureNames = ["season","holiday","workingday","weather","weekday","month","year","hour"]
numFeatureNames = ["temp","humidity","windspeed","atemp"]
dropFeatures = ['casual',"count","datetime","date","registered","year"]

for var in catFeatureNames:
    data_final[var] = data_final[var].astype("category")

datatrain = data_final[pd.notnull(data_final['count'])].sort_values(by=["datetime"])
datatest = data_final[~pd.notnull(data_final['count'])].sort_values(by=["datetime"])
#datetimecol = data_final["datetime"]
yLabels = datatrain["count"]
yLablesRegistered = datatrain["registered"]
yLablesCasual = datatrain["casual"]

datatrain  = datatrain.drop(dropFeatures,axis=1)
datatest  = datatest.drop(dropFeatures,axis=1)

datatrain

def rmsle(y, y1,convertExp=True):
    if convertExp:
        y = np.exp(y),
        y1 = np.exp(y1)
    log1 = np.nan_to_num(np.array([np.log(x + 1) for x in y]))
    log2 = np.nan_to_num(np.array([np.log(x + 1) for x in y1]))
    calc = (log1 - log2) ** 2
    return np.sqrt(np.mean(calc))

from sklearn.linear_model import LinearRegression
from sklearn import metrics
lm = LinearRegression()
yLabelsLog = np.log1p(yLabels)
lm.fit(X = datatrain,y = yLabelsLog)
preds_lm = lm.predict(datatrain)
rmsle_preds_lm = rmsle(np.exp(yLabelsLog),np.exp(preds_lm),False)
preds_Test_lm = lm.predict(X= datatest)

from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor(n_estimators=100)
yLabelsLog = np.log1p(yLabels)
rf.fit(datatrain,yLabelsLog)
preds_rf = rf.predict(datatrain)
rmsle_preds_rf = rmsle(np.exp(yLabelsLog),np.exp(preds_rf),False)
preds_Test_rf = rf.predict(X= datatest)

from sklearn.ensemble import GradientBoostingRegressor
gbm = GradientBoostingRegressor(n_estimators=4000,alpha=0.01);
yLabelsLog = np.log1p(yLabels)
gbm.fit(datatrain,yLabelsLog)
preds_gbm = gbm.predict(datatrain)
rmsle_preds_gbm = rmsle(np.exp(yLabelsLog),np.exp(preds_gbm),False)
preds_Test_gbm = gbm.predict(X= datatest)

dataf = {
        'RMSLE': [rmsle_preds_lm, rmsle_preds_rf,rmsle_preds_gbm],
        'Model':['Linear Regression','RandomForest','GBM'],}
final_results = pd.DataFrame(data=dataf)
final_results

## we can see that the random forest model works the best with the least RMSLE Value

fig,(ax1,ax2)= plt.subplots(ncols=2)
fig.set_size_inches(12,5)
sn.distplot(yLabels,ax=ax1,bins=50)
sn.distplot(np.exp(preds_Test_gbm),ax=ax2,bins=50)

"""Splitting training and testing"""

datatrain1= data_final[pd.notnull(data_final['count'])].sort_values(by=["datetime"])
dropFeatures1 = ['casual',"datetime","date","registered","year"]
datatrain1 =datatrain1.drop(dropFeatures1 , axis = 1)

X1 = datatrain1.drop('count', axis =1)
y1 = datatrain1.iloc[:,1].values

from sklearn.model_selection import train_test_split
X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size = 0.20,random_state = 0)

def mean_absolute_percentage_error(y_true, y_pred): 
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100

lm.fit(X_train1, y_train1)
y_pred_regr= lm.predict(X_test1)
MAPE_reg = mean_absolute_percentage_error(y_pred_regr, y_test1)
MAE_reg = metrics.mean_absolute_error(y_pred_regr, y_test1)
##RF
rf.fit(X_train1, y_train1)
y_pred_rf= rf.predict(X_test1)
MAPE_rf = mean_absolute_percentage_error(y_pred_rf, y_test1)
MAE_rf = metrics.mean_absolute_error(y_pred_rf, y_test1)
##GBM
gbm.fit(X_train1, y_train1)
y_pred_gbm= gbm.predict(X_test1)
MAPE_gbm = mean_absolute_percentage_error(y_pred_gbm, y_test1)
MAE_gbm = metrics.mean_absolute_error(y_pred_gbm, y_test1)

dataf = {
        'MAE': [MAE_reg, MAE_rf,MAE_gbm],
        'MAPE': [MAPE_reg,MAPE_rf,MAPE_gbm],
        'Model':['Linear Regression','RandomForest','GBM'],}
final_metrics = pd.DataFrame(data=dataf)
final_metrics

"""**Random Forest had an accuracy of 72% and is our best model**"""